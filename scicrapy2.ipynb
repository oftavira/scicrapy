{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class ProxyInList:\n",
    "\n",
    "    def __init__(self, ip, port, country, anonymity, google, https, last_checked):\n",
    "        self.ip = ip\n",
    "        self.port = port\n",
    "        self.country = country\n",
    "        self.anonymity = anonymity\n",
    "        self.google = google\n",
    "        self.https = https\n",
    "        self.last_checked = last_checked\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'P: ' + self.ip + ':' + self.port + ' ' + 'C: ' + self.country + ' ' + 'A: ' +self.anonymity + ' ' + 'G: ' +self.google + ' ' + 'HTTPS: ' +self.https + ' ' + 'T: ' + self.last_checked\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'P: ' + self.ip + ':' + self.port + ' ' + 'C: ' + self.country + ' ' + 'A: ' +self.anonymity + ' ' + 'G: ' +self.google + ' ' + 'HTTPS: ' +self.https + ' ' + 'T: ' + self.last_checked\n",
    "    \n",
    "    # Now we set a getter for the ip and port, this allows to obtain the ip and port without calling a method and instead just calling the variable\n",
    "\n",
    "    @property\n",
    "    def ip_port(self):\n",
    "        return self.ip + ':' + self.port\n",
    "\n",
    "\n",
    "\n",
    "class Proxies:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.proxies = []\n",
    "        self.get_proxies()\n",
    "\n",
    "    def get_proxies(self):\n",
    "        proxy_page = requests.get('https://free-proxy-list.net/')\n",
    "        soup_prox = BeautifulSoup(proxy_page.content, 'html.parser')\n",
    "        prox = soup_prox.find('div', class_='table-responsive fpl-list')\n",
    "        prox2 = prox.find('table')\n",
    "        prox3 = prox2.find('tbody')\n",
    "        prox4 = prox3.find_all('tr')\n",
    "        for i in prox4:\n",
    "            atributes = i.find_all('td')\n",
    "            self.proxies.append(ProxyInList(atributes[0].text, atributes[1].text, atributes[2].text, atributes[4].text, atributes[5].text, atributes[6].text, atributes[7].text))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.proxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "prox = Proxies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'47.241.165.133:443'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prox.proxies[0].ip_port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class GiveMeProxy:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.proxies = []\n",
    "        self.get_proxies()\n",
    "    \n",
    "    def get_proxies(self):\n",
    "        proxy_page = requests.get('https://free-proxy-list.net/')\n",
    "        soup_prox = BeautifulSoup(proxy_page.content, 'html.parser')\n",
    "        prox = soup_prox.find('div', class_='table-responsive fpl-list')\n",
    "        prox = prox.find('table')\n",
    "        prox = prox.find('tbody')\n",
    "        prox = prox.find_all('tr')\n",
    "        for i in prox:\n",
    "            self.proxies.append(i.find('td').text)\n",
    "        return self.proxies\n",
    "\n",
    "proxy_page = requests.get('https://free-proxy-list.net/')\n",
    "soup_prox = BeautifulSoup(proxy_page.content, 'html.parser')\n",
    "prox = soup_prox.find('div', class_='table-responsive fpl-list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = prox.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "var1 = ps[0].find_all('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IP Address\n",
      "Port\n",
      "Code\n",
      "Country\n",
      "Anonymity\n",
      "Google\n",
      "Https\n",
      "Last Checked\n"
     ]
    }
   ],
   "source": [
    "for cont in var1:\n",
    "    print(cont.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tr><th>IP Address</th><th>Port</th><th>Code</th><th class=\"hm\">Country</th><th>Anonymity</th><th class=\"hm\">Google</th><th class=\"hx\">Https</th><th class=\"hm\">Last Checked</th></tr>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.186.239.244\n",
      "8080\n",
      "NL\n",
      "Netherlands\n",
      "elite proxy\n",
      "no\n",
      "yes\n",
      "5 secs ago\n"
     ]
    }
   ],
   "source": [
    "var2 = ps[0].find_all('td')\n",
    "\n",
    "for cont in var2:\n",
    "    print(cont.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ch), len(ch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "proxies = {\n",
    "   'http': 'http://195.8.249.242:80',\n",
    "}\n",
    "\n",
    "class SciScraper2(object):\n",
    "\n",
    "    def __init__(self, term):\n",
    "        self.term = term\n",
    "        self.history = []\n",
    "        \n",
    "        # self.url ='https://iopscience.iop.org/nsearch?terms=raman&nextPage=2&previousPage=-1&currentPage=1&searchDatePeriod=anytime&orderBy=relevance&pageLength=50'\n",
    "        self.url = 'https://en.wikipedia.org/wiki/Structural_formula'\n",
    "\n",
    "        # TODO : Change the headers dybamically\n",
    "\n",
    "        user_agent1 = 'Mac OS X10/Safari browser: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9'\n",
    "        user_agent2 = 'Windows 10/Chrome browser: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'\n",
    "        user_agent3 = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36'\n",
    "        \n",
    "        header = {\"User-Agent\": user_agent2}\n",
    "\n",
    "        # TODO : Manage history of searched terms\n",
    "        self.current = requests.get(self.url, headers = header, proxies=proxies)\n",
    "        self.history.append(self.current)\n",
    "    \n",
    "    def construct_page(self):\n",
    "        # TODO : Construct the page\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrap = SciScraper2('raman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrap.current.text\n",
    "\n",
    "# Now we save the html file\n",
    "\n",
    "with open('scicrapy2.html', 'w') as f:\n",
    "    f.write(scrap.current.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In this notebook, we will create a scrapping tool for scientific articles\n",
    "\n",
    "# Importing the libraries\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url ='https://iopscience.iop.org/nsearch?terms=raman&nextPage=2&previousPage=-1&currentPage=1&searchDatePeriod=anytime&orderBy=relevance&pageLength=50'\n",
    "\n",
    "# Getting the webpage, with headers to make sure we get the results we require\n",
    "# We create the headers as a dictionary\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\"}\n",
    "page = requests.get(url, headers=headers)\n",
    "\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "arts = soup.find_all('div', class_=\"art-list-item-body\")\n",
    "\n",
    "len(arts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not Response",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/jupyter/solt/scicrapy2/scicrapy/scicrapy2.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jupyter/solt/scicrapy2/scicrapy/scicrapy2.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mpage.html\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jupyter/solt/scicrapy2/scicrapy/scicrapy2.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         file\u001b[39m.\u001b[39mwrite(page)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jupyter/solt/scicrapy2/scicrapy/scicrapy2.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m save(page\u001b[39m=\u001b[39;49mpage)\n",
      "\u001b[1;32m/home/jupyter/solt/scicrapy2/scicrapy/scicrapy2.ipynb Cell 3\u001b[0m in \u001b[0;36msave\u001b[0;34m(page)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jupyter/solt/scicrapy2/scicrapy/scicrapy2.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave\u001b[39m(page):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jupyter/solt/scicrapy2/scicrapy/scicrapy2.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mpage.html\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jupyter/solt/scicrapy2/scicrapy/scicrapy2.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         file\u001b[39m.\u001b[39;49mwrite(page)\n",
      "\u001b[0;31mTypeError\u001b[0m: write() argument must be str, not Response"
     ]
    }
   ],
   "source": [
    "# Saving this page as an html file\n",
    "\n",
    "def save(page):\n",
    "    with open('page.html', 'w') as file:\n",
    "        file.write(page)\n",
    "\n",
    "save(page=page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We select an article to organize our tool\n",
    "\n",
    "art = arts[1]\n",
    "\n",
    "# We get the title and all other information and set it to a dictionary\n",
    "# title = art.find('h2', class_=\"art-list-item-title\")\n",
    "# Now we replace the following characters in the title with nothing \\r \\n and double spaces\n",
    "# r = ''\n",
    "# for char in title.text:\n",
    "#     if char in ['\\r', '\\n']:\n",
    "#         pass\n",
    "#     else:\n",
    "#         r += char\n",
    "# r = r.replace('  ', '')\n",
    "# r\n",
    "\n",
    "\n",
    "\n",
    "# authors = art.find_all('span', itemprop=\"author\")\n",
    "# auths = []\n",
    "# for a in authors:\n",
    "#     auths.append(a.text)\n",
    "# auths\n",
    "\n",
    "\n",
    "\n",
    "# journal = art.find_all('p', class_=\"small art-list-item-meta\")\n",
    "\n",
    "\n",
    "\n",
    "# date = art.find('div', class_=\"art-list-item-date\").text\n",
    "\n",
    "\n",
    "\n",
    "# abstract = art.find('div', class_=\"article-text view-text-small\").text.strip()\n",
    "# # <div class=\"reveal-content\">\n",
    "# # <div class=\"article-text view-text-small\">\n",
    "# abstract\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# link = art.find('a', class_=\"art-list-item-link\")['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'text'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/jupyter/solt/scicrapy2/scicrapy/scicrapy2.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jupyter/solt/scicrapy2/scicrapy/scicrapy2.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m abstract\u001b[39m.\u001b[39;49mtext\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/bs4/element.py:2289\u001b[0m, in \u001b[0;36mResultSet.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[1;32m   2288\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Raise a helpful exception to explain a common code fix.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2289\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m   2290\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mResultSet object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. You\u001b[39m\u001b[39m'\u001b[39m\u001b[39mre probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m key\n\u001b[1;32m   2291\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: ResultSet object has no attribute 'text'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "abstract.text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Dec 19 2022, 17:35:49) [GCC 12.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
