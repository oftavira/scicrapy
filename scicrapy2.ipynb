{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "class ProxyInList:\n",
    "\n",
    "    def __init__(self, ip, port, country, anonymity, google, https, last_checked):\n",
    "        self.ip = ip\n",
    "        self.port = port\n",
    "        self.country = country\n",
    "        self.anonymity = anonymity\n",
    "        self.google = google\n",
    "        self.https = https\n",
    "        self.last_checked = last_checked\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'ip:port: ' + self.ip + ':' + self.port + ' ' + 'country: ' + self.country + ' ' + 'anonymity: ' +self.anonymity + ' ' + 'google: ' +self.google + ' ' + 'https: ' +self.https + ' ' + 'last_checked: ' + self.last_checked\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'ip:port: ' + self.ip + ':' + self.port + ' ' + 'country: ' + self.country + ' ' + 'anonymity: ' +self.anonymity + ' ' + 'google: ' +self.google + ' ' + 'https: ' +self.https + ' ' + 'last_checked: ' + self.last_checked\n",
    "    \n",
    "    # Now we set a getter for the ip and port, this allows to obtain the ip and port without calling a method and instead just calling the variable\n",
    "\n",
    "    @property\n",
    "    def ip_port(self):\n",
    "        return self.ip + ':' + self.port\n",
    "\n",
    "\n",
    "\n",
    "class Proxies:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.proxies = []\n",
    "        self.get_proxies()\n",
    "\n",
    "    def get_proxies(self):\n",
    "        proxy_page = requests.get('https://free-proxy-list.net/')\n",
    "        soup_prox = BeautifulSoup(proxy_page.content, 'html.parser')\n",
    "        prox = soup_prox.find('div', class_='table-responsive fpl-list')\n",
    "        prox2 = prox.find('table')\n",
    "        prox3 = prox2.find('tbody')\n",
    "        prox4 = prox3.find_all('tr')\n",
    "        for i in prox4:\n",
    "            atributes = i.find_all('td')\n",
    "            self.proxies.append(ProxyInList(atributes[0].text, atributes[1].text, atributes[2].text, atributes[4].text, atributes[5].text, atributes[6].text, atributes[7].text))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.proxies)\n",
    "    \n",
    "    @property\n",
    "    def withHttps(self):\n",
    "        listed = [x for x in self.proxies if x.https == 'yes']\n",
    "        lss = []\n",
    "        for e in listed:\n",
    "            lss.append(e.ip_port)\n",
    "        return lss\n",
    "        \n",
    "    @property\n",
    "    def giveMeProxies(self):\n",
    "        lss = self.withHttps\n",
    "        return random.choice(lss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Proxies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = p.giveMeProxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'143.198.213.77:1080'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://link.springer.com/search?query=raman Mac OS X10/Safari browser: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9\n",
      "Here is the psr... 118.27.113.167:8080\n",
      "Error with requests\n",
      "Error with urllib\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "url1 = 'https://iopscience.iop.org/nsearch?terms=raman&nextPage=2&previousPage=-1&currentPage=1&searchDatePeriod=anytime&orderBy=relevance&pageLength=50'\n",
    "url2 = 'https://www.nature.com/search?q=raman+data+science&order=relevance&page=2'\n",
    "url3 = 'https://link.springer.com/search?query=raman'\n",
    "wiki = 'https://en.wikipedia.org/wiki/Python_(programming_language)'\n",
    "\n",
    "urls = [url1, url2, url3]\n",
    "\n",
    "user_agent1 = 'Mac OS X10/Safari browser: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9'\n",
    "user_agent2 = 'Windows 10/Chrome browser: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'\n",
    "user_agent3 = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36'\n",
    "\n",
    "user_agents = [user_agent1, user_agent2, user_agent3]\n",
    "\n",
    "random_user_agent = user_agents[randint(0, len(user_agents)-1)]\n",
    "\n",
    "random_url = urls[randint(0, len(urls)-1)]\n",
    "print(random_url, random_user_agent)\n",
    "\n",
    "# In the following code we try to make a request to the page using three\n",
    "# different modules: requests, urllib and selenium\n",
    "\n",
    "import requests\n",
    "import urllib.request\n",
    "from selenium import webdriver\n",
    "\n",
    "##\n",
    "pro = Proxies()\n",
    "spr = pro.giveMeProxies\n",
    "##\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "proxy = spr\n",
    "\n",
    "print('Here is the psr...', proxy)\n",
    "\n",
    "\n",
    "##\n",
    "\n",
    "\n",
    "# Selecting a random ip and port from the list of proxies\n",
    "\n",
    "def req_only_requests(url, user_agent):\n",
    "    headers = {'User-Agent': user_agent}\n",
    "    proxy = spr\n",
    "    page = requests.get(url, headers=headers, proxies={'https': proxy})\n",
    "    print(page.status_code)\n",
    "\n",
    "def req_only_urllib(url, user_agent):\n",
    "    req = urllib.request.Request(url, headers={'User-Agent': user_agent}, proxy=proxy)\n",
    "    page = urllib.request.urlopen(req)\n",
    "    print(page.status)\n",
    "try:\n",
    "    req_only_requests(wiki, random_user_agent)\n",
    "except:\n",
    "    print('Error with requests')\n",
    "try:\n",
    "    req_only_urllib(wiki, random_user_agent)\n",
    "except:\n",
    "    print('Error with urllib')\n",
    "\n",
    "# def requesting_with_three_modules(url, user_agent):\n",
    "\n",
    "#     headers = {'User-Agent': user_agent}\n",
    "#     pro = Proxies()\n",
    "#     proxy = pro.giveMeProxies().ip_port\n",
    "\n",
    "    \n",
    "#     page = requests.get(url, headers=headers, proxies={'http': proxy, 'https': proxy})\n",
    "#     print(page.status_code)\n",
    "\n",
    "    \n",
    "\n",
    "#     req = urllib.request.Request(url, headers={'User-Agent': user_agent}, proxy=proxy)\n",
    "#     page = urllib.request.urlopen(req)\n",
    "#     print(page.status)\n",
    "\n",
    "    \n",
    "\n",
    "#     chrome_options = webdriver.ChromeOptions()\n",
    "#     chrome_options.add_argument(\"--proxy-server={}\".format(proxy))\n",
    "#     chrome_options.add_argument(\"--user-agent={}\".format(random_user_agent))\n",
    "#     driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "#     driver.get(url)\n",
    "#     print(driver.title)\n",
    "#     driver.close()\n",
    "\n",
    "# requesting_with_three_modules(wiki, random_user_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class SciScraper2(object):\n",
    "\n",
    "    def __init__(self, term):\n",
    "        self.prox = Proxies()\n",
    "        self.term = term\n",
    "        self.history = []\n",
    "        \n",
    "        # self.url ='https://iopscience.iop.org/nsearch?terms=raman&nextPage=2&previousPage=-1&currentPage=1&searchDatePeriod=anytime&orderBy=relevance&pageLength=50'\n",
    "        self.url = 'https://en.wikipedia.org/wiki/Structural_formula'\n",
    "\n",
    "        # TODO : Change the headers dybamically\n",
    "\n",
    "        user_agent1 = 'Mac OS X10/Safari browser: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9'\n",
    "        user_agent2 = 'Windows 10/Chrome browser: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'\n",
    "        user_agent3 = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36'\n",
    "        \n",
    "        header = {\"User-Agent\": user_agent2}\n",
    "\n",
    "        # TODO : Manage history of searched terms\n",
    "        self.current = requests.get(self.url, headers = header, proxies=proxies)\n",
    "        self.history.append(self.current)\n",
    "    \n",
    "    def construct_page(self):\n",
    "        # TODO : Construct the page\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://35.232.47.164:8005\n"
     ]
    },
    {
     "ename": "SSLError",
     "evalue": "HTTPSConnectionPool(host='en.wikipedia.org', port=443): Max retries exceeded with url: /wiki/Structural_formula (Caused by SSLError(SSLZeroReturnError(6, 'TLS/SSL connection has been closed (EOF) (_ssl.c:997)')))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSSLZeroReturnError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/urllib3/connectionpool.py:700\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[39mif\u001b[39;00m is_new_proxy_conn \u001b[39mand\u001b[39;00m http_tunnel_required:\n\u001b[0;32m--> 700\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/urllib3/connectionpool.py:996\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._prepare_proxy\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    994\u001b[0m     conn\u001b[39m.\u001b[39mtls_in_tls_required \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 996\u001b[0m conn\u001b[39m.\u001b[39;49mconnect()\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/urllib3/connection.py:364\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtls_in_tls_required:\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connect_tls_proxy(hostname, conn)\n\u001b[1;32m    365\u001b[0m     tls_in_tls \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/urllib3/connection.py:499\u001b[0m, in \u001b[0;36mHTTPSConnection._connect_tls_proxy\u001b[0;34m(self, hostname, conn)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39m# If no cert was provided, use only the default options for server\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[39m# certificate validation\u001b[39;00m\n\u001b[0;32m--> 499\u001b[0m socket \u001b[39m=\u001b[39m ssl_wrap_socket(\n\u001b[1;32m    500\u001b[0m     sock\u001b[39m=\u001b[39;49mconn,\n\u001b[1;32m    501\u001b[0m     ca_certs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_certs,\n\u001b[1;32m    502\u001b[0m     ca_cert_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_dir,\n\u001b[1;32m    503\u001b[0m     ca_cert_data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_data,\n\u001b[1;32m    504\u001b[0m     server_hostname\u001b[39m=\u001b[39;49mhostname,\n\u001b[1;32m    505\u001b[0m     ssl_context\u001b[39m=\u001b[39;49mssl_context,\n\u001b[1;32m    506\u001b[0m )\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m ssl_context\u001b[39m.\u001b[39mverify_mode \u001b[39m!=\u001b[39m ssl\u001b[39m.\u001b[39mCERT_NONE \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(\n\u001b[1;32m    509\u001b[0m     ssl_context, \u001b[39m\"\u001b[39m\u001b[39mcheck_hostname\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    510\u001b[0m ):\n\u001b[1;32m    511\u001b[0m     \u001b[39m# While urllib3 attempts to always turn off hostname matching from\u001b[39;00m\n\u001b[1;32m    512\u001b[0m     \u001b[39m# the TLS library, this cannot always be done. So we check whether\u001b[39;00m\n\u001b[1;32m    513\u001b[0m     \u001b[39m# the TLS Library still thinks it's matching hostnames.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/urllib3/util/ssl_.py:453\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 453\u001b[0m     ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(sock, context, tls_in_tls)\n\u001b[1;32m    454\u001b[0m \u001b[39mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/urllib3/util/ssl_.py:495\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 495\u001b[0m     \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39;49mwrap_socket(sock)\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:513\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    508\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    509\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    510\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    511\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    512\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 513\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msslsocket_class\u001b[39m.\u001b[39;49m_create(\n\u001b[1;32m    514\u001b[0m         sock\u001b[39m=\u001b[39;49msock,\n\u001b[1;32m    515\u001b[0m         server_side\u001b[39m=\u001b[39;49mserver_side,\n\u001b[1;32m    516\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39;49mdo_handshake_on_connect,\n\u001b[1;32m    517\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39;49msuppress_ragged_eofs,\n\u001b[1;32m    518\u001b[0m         server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    519\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    520\u001b[0m         session\u001b[39m=\u001b[39;49msession\n\u001b[1;32m    521\u001b[0m     )\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1071\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1071\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1072\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1342\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1341\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1342\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1343\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[0;31mSSLZeroReturnError\u001b[0m: TLS/SSL connection has been closed (EOF) (_ssl.c:997)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m     )\n\u001b[1;32m    502\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[0;32m--> 787\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    788\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    789\u001b[0m )\n\u001b[1;32m    790\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/urllib3/util/retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[39mif\u001b[39;00m new_retry\u001b[39m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 592\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[39mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    594\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='en.wikipedia.org', port=443): Max retries exceeded with url: /wiki/Structural_formula (Caused by SSLError(SSLZeroReturnError(6, 'TLS/SSL connection has been closed (EOF) (_ssl.c:997)')))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/jupyter/solt/scicrapy2/scicrapy/scicrapy2.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jupyter/solt/scicrapy2/scicrapy/scicrapy2.ipynb#X16sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m ua3 \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jupyter/solt/scicrapy2/scicrapy/scicrapy2.ipynb#X16sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m headers \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mUser-Agent\u001b[39m\u001b[39m\"\u001b[39m: ua1}\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jupyter/solt/scicrapy2/scicrapy/scicrapy2.ipynb#X16sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m page \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(url2, headers\u001b[39m=\u001b[39;49mheaders, proxies\u001b[39m=\u001b[39;49mproxies)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jupyter/solt/scicrapy2/scicrapy/scicrapy2.ipynb#X16sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(page\u001b[39m.\u001b[39mcontent, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jupyter/solt/scicrapy2/scicrapy/scicrapy2.ipynb#X16sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m# arts = soup.find_all('div', class_=\"art-list-item-body\")\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/requests/adapters.py:563\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[39mraise\u001b[39;00m ProxyError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    561\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    562\u001b[0m         \u001b[39m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m--> 563\u001b[0m         \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    565\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    567\u001b[0m \u001b[39mexcept\u001b[39;00m ClosedPoolError \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mSSLError\u001b[0m: HTTPSConnectionPool(host='en.wikipedia.org', port=443): Max retries exceeded with url: /wiki/Structural_formula (Caused by SSLError(SSLZeroReturnError(6, 'TLS/SSL connection has been closed (EOF) (_ssl.c:997)')))"
     ]
    }
   ],
   "source": [
    "# In this notebook, we will create a scrapping tool for scientific articles\n",
    "# Importing the libraries\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "p = Proxies()\n",
    "# Creating a random proxy picking a random element from the list of proxies\n",
    "index = len(p.withHttps())\n",
    "\n",
    "# Picking a random number between 0 and the length of the list of proxies\n",
    "import random\n",
    "random_index = random.randint(0, index)\n",
    "pot = 'https://'+ p.withHttps()[random_index].ip_port\n",
    "\n",
    "print(pot)\n",
    "\n",
    "url1 ='https://iopscience.iop.org/nsearch?terms=raman&nextPage=2&previousPage=-1&currentPage=1&searchDatePeriod=anytime&orderBy=relevance&pageLength=50'\n",
    "url2 = 'https://en.wikipedia.org/wiki/Structural_formula'\n",
    "\n",
    "# Getting the webpage, with headers to make sure we get the results we require\n",
    "# We create the headers as a dictionary\n",
    "\n",
    "proxies = {\n",
    "   'http': pot,\n",
    "}\n",
    "\n",
    "ua1 = 'Mac OS X10/Safari browser: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9'\n",
    "ua2 = 'Windows 10/Chrome browser: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'\n",
    "ua3 = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36'\n",
    "\n",
    "headers = {\"User-Agent\": ua1}\n",
    "page = requests.get(url2, headers=headers, proxies=proxies)\n",
    "\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "# arts = soup.find_all('div', class_=\"art-list-item-body\")\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not Response",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/jupyter/solt/scicrapy2/scicrapy/scicrapy2.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jupyter/solt/scicrapy2/scicrapy/scicrapy2.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mpage.html\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jupyter/solt/scicrapy2/scicrapy/scicrapy2.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         file\u001b[39m.\u001b[39mwrite(page)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jupyter/solt/scicrapy2/scicrapy/scicrapy2.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m save(page\u001b[39m=\u001b[39;49mpage)\n",
      "\u001b[1;32m/home/jupyter/solt/scicrapy2/scicrapy/scicrapy2.ipynb Cell 3\u001b[0m in \u001b[0;36msave\u001b[0;34m(page)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jupyter/solt/scicrapy2/scicrapy/scicrapy2.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave\u001b[39m(page):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jupyter/solt/scicrapy2/scicrapy/scicrapy2.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mpage.html\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jupyter/solt/scicrapy2/scicrapy/scicrapy2.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         file\u001b[39m.\u001b[39;49mwrite(page)\n",
      "\u001b[0;31mTypeError\u001b[0m: write() argument must be str, not Response"
     ]
    }
   ],
   "source": [
    "# Saving this page as an html file\n",
    "\n",
    "def save(page):\n",
    "    with open('page.html', 'w') as file:\n",
    "        file.write(page)\n",
    "\n",
    "save(page=page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We select an article to organize our tool\n",
    "\n",
    "art = arts[1]\n",
    "\n",
    "# We get the title and all other information and set it to a dictionary\n",
    "# title = art.find('h2', class_=\"art-list-item-title\")\n",
    "# Now we replace the following characters in the title with nothing \\r \\n and double spaces\n",
    "# r = ''\n",
    "# for char in title.text:\n",
    "#     if char in ['\\r', '\\n']:\n",
    "#         pass\n",
    "#     else:\n",
    "#         r += char\n",
    "# r = r.replace('  ', '')\n",
    "# r\n",
    "\n",
    "\n",
    "\n",
    "# authors = art.find_all('span', itemprop=\"author\")\n",
    "# auths = []\n",
    "# for a in authors:\n",
    "#     auths.append(a.text)\n",
    "# auths\n",
    "\n",
    "\n",
    "\n",
    "# journal = art.find_all('p', class_=\"small art-list-item-meta\")\n",
    "\n",
    "\n",
    "\n",
    "# date = art.find('div', class_=\"art-list-item-date\").text\n",
    "\n",
    "\n",
    "\n",
    "# abstract = art.find('div', class_=\"article-text view-text-small\").text.strip()\n",
    "# # <div class=\"reveal-content\">\n",
    "# # <div class=\"article-text view-text-small\">\n",
    "# abstract\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# link = art.find('a', class_=\"art-list-item-link\")['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'text'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/jupyter/solt/scicrapy2/scicrapy/scicrapy2.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jupyter/solt/scicrapy2/scicrapy/scicrapy2.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m abstract\u001b[39m.\u001b[39;49mtext\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/bs4/element.py:2289\u001b[0m, in \u001b[0;36mResultSet.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[1;32m   2288\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Raise a helpful exception to explain a common code fix.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2289\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m   2290\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mResultSet object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. You\u001b[39m\u001b[39m'\u001b[39m\u001b[39mre probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m key\n\u001b[1;32m   2291\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: ResultSet object has no attribute 'text'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "abstract.text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
